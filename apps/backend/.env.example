# ============================================
# AI Speech Evaluator Backend Configuration
# ============================================

# Server Configuration
PORT=3001
NODE_ENV=development

# Database (Optional - currently using in-memory storage)
# DATABASE_URL=postgresql://user:password@localhost:5432/ai_speech_evaluator

# Redis (Optional - for session management and caching)
# REDIS_URL=redis://localhost:6379

# ============================================
# LOCAL AI CONFIGURATION (OLLAMA)
# ============================================

# Ollama Server Configuration
OLLAMA_HOST=http://localhost:11434
OLLAMA_MODEL=llama3.2:3b
OLLAMA_EMBEDDING_MODEL=nomic-embed-text
OLLAMA_TIMEOUT=120000

# Local Whisper Configuration
WHISPER_MODEL_PATH=./models/whisper
WHISPER_MODEL=base
WHISPER_LANGUAGE=auto

# ============================================
# VECTOR DATABASE CONFIGURATION
# ============================================

# Vector Database Type: 'memory' (default) or 'pinecone'
VECTOR_DB_TYPE=memory

# Pinecone Configuration (only if VECTOR_DB_TYPE=pinecone)
# PINECONE_API_KEY=your_pinecone_api_key
# PINECONE_ENVIRONMENT=your_pinecone_environment
# PINECONE_INDEX_NAME=ai-speech-evaluator

# Embedding Configuration
EMBEDDING_DIMENSIONS=768
EMBEDDING_BATCH_SIZE=5

# ============================================
# DOCUMENT PROCESSING CONFIGURATION
# ============================================

# Chunking Configuration
CHUNK_SIZE=1000
CHUNK_OVERLAP=200
PRESERVE_SENTENCES=true
PRESERVE_PARAGRAPHS=false

# ============================================
# EVALUATION CONFIGURATION
# ============================================

# Evaluation Model Configuration
EVALUATION_MODEL=llama3.2:3b
EVALUATION_TEMPERATURE=0.3
EVALUATION_MAX_TOKENS=2000
EVALUATION_LANGUAGE=italian

# ============================================
# FILE STORAGE CONFIGURATION
# ============================================

# Upload Configuration
UPLOAD_DIR=./uploads
MAX_FILE_SIZE=50
MAX_AUDIO_DURATION=600
AUDIO_SAMPLE_RATE=16000

# ============================================
# SECURITY & CORS CONFIGURATION
# ============================================

# CORS Configuration
FRONTEND_URL=http://localhost:3000

# Logging Configuration
LOG_LEVEL=info
LOG_FORMAT=combined
LOG_DIR=./logs

# ============================================
# DEVELOPMENT TOOLS
# ============================================

# Set to true to enable test endpoints in development
ENABLE_TEST_ENDPOINTS=true

# ============================================
# INSTALLATION INSTRUCTIONS
# ============================================

# 1. Install Ollama:
#    Visit https://ollama.ai and download for your platform
#
# 2. Pull required models:
#    ollama pull llama3.2:3b
#    ollama pull nomic-embed-text
#
# 3. Start Ollama server:
#    ollama serve
#
# 4. Copy this file to .env and adjust values as needed:
#    cp .env.example .env
#
# 5. Install Whisper (optional for local transcription):
#    pip install openai-whisper
#    # or use whisper.cpp for better performance
#
# 6. Start the development server:
#    npm run dev

# ============================================
# PRODUCTION NOTES
# ============================================

# For production deployment:
# 1. Set NODE_ENV=production
# 2. Configure DATABASE_URL if using persistent storage
# 3. Set appropriate LOG_LEVEL (warn or error)
# 4. Configure CORS with your actual frontend domains
# 5. Consider using Pinecone for vector storage in production
# 6. Ensure Ollama server is properly configured and secured