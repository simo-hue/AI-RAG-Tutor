# Configurazione Ollama - AI Speech Evaluator

## ‚ú® Novit√†: Avvio Automatico e Gestione Modelli

Il sistema ora avvia automaticamente Ollama e permette di selezionare il modello AI da utilizzare direttamente dall'interfaccia web!

## üöÄ Funzionalit√† Implementate

### 1. Avvio Automatico di Ollama
Quando avvii l'applicazione con `npm run dev`, il backend:
- ‚úÖ Verifica se Ollama √® installato
- ‚úÖ Avvia automaticamente il servizio Ollama se non √® gi√† in esecuzione
- ‚úÖ Controlla la disponibilit√† del modello predefinito (`llama3.2:3b`)
- ‚úÖ Mostra messaggi informativi nella console

### 2. Interfaccia di Gestione Modelli
Nella pagina `/upload` troverai un nuovo componente che ti permette di:
- üìã Visualizzare tutti i modelli disponibili
- ‚¨áÔ∏è Scaricare nuovi modelli direttamente dall'interfaccia
- ‚úÖ Selezionare quale modello utilizzare per le valutazioni
- üìä Vedere lo stato di download in tempo reale

### 3. Modelli Consigliati

Il sistema suggerisce i seguenti modelli:

| Modello | Dimensione | Descrizione | Consigliato |
|---------|-----------|-------------|-------------|
| **llama3.2:3b** | ~2GB | Veloce e leggero, ideale per la maggior parte dei casi | ‚úÖ |
| llama3.2:1b | ~1GB | Molto leggero, ottimo per test rapidi | |
| llama3.1:8b | ~5GB | Pi√π potente ma richiede pi√π risorse | |
| gemma2:2b | ~1.5GB | Alternativa efficiente di Google | |

## üì¶ Installazione Iniziale

Se Ollama non √® ancora installato:

```bash
# macOS/Linux
curl -fsSL https://ollama.ai/install.sh | sh

# Oppure scarica da
# https://ollama.ai
```

## üéØ Come Utilizzare

### Avvio Rapido

1. **Avvia l'applicazione:**
   ```bash
   npm run dev
   ```

2. **Il sistema automaticamente:**
   - Avvia Ollama (se installato)
   - Verifica i modelli disponibili
   - Mostra lo stato nella console

3. **Vai su http://localhost:3000/upload**
   - Vedrai il selettore di modelli in alto
   - Scarica il modello consigliato se necessario
   - Seleziona il modello da utilizzare

### Download di un Modello

Due modi per scaricare un modello:

**Metodo 1: Dall'interfaccia web (Consigliato)**
1. Vai su `/upload`
2. Trova il modello nel selettore
3. Clicca "Scarica"
4. Attendi il completamento (verr√† mostrata una progress bar)

**Metodo 2: Da terminale**
```bash
ollama pull llama3.2:3b
```

## üîß API Endpoints Disponibili

Il backend ora espone questi nuovi endpoint:

```bash
# Stato del servizio Ollama
GET /api/ollama/status

# Avvia Ollama
POST /api/ollama/start

# Lista modelli disponibili
GET /api/ollama/models

# Scarica un modello
POST /api/ollama/models/pull
Body: { "modelName": "llama3.2:3b" }

# Verifica disponibilit√† modello
POST /api/ollama/models/ensure
Body: { "modelName": "llama3.2:3b" }
```

## üîç Verifica Installazione

Per verificare che tutto funzioni:

```bash
# 1. Verifica che Ollama sia installato
which ollama

# 2. Verifica che il servizio sia attivo
curl http://localhost:11434/api/tags

# 3. Lista modelli installati
ollama list

# 4. Test del backend
curl http://localhost:3002/api/ollama/status
```

## üìù Esempio di Utilizzo Completo

```bash
# 1. Avvia l'applicazione
npm run dev

# Output atteso:
# ‚úÖ Ollama service is running
# ‚úÖ Default model llama3.2:3b is available
# üöÄ Server running on port 3002
# ü§ñ Ollama API: http://localhost:3002/api/ollama

# 2. Apri il browser
# http://localhost:3000/upload

# 3. Nel selettore modelli:
# - Se llama3.2:3b √® gi√† installato: vedrai il badge "Installato"
# - Se non √® installato: clicca "Scarica" e attendi

# 4. Carica un documento e registra l'audio

# 5. La valutazione utilizzer√† automaticamente il modello selezionato
```

## üõ†Ô∏è Risoluzione Problemi

### Ollama non si avvia automaticamente

**Sintomi:**
- Console mostra "‚ö†Ô∏è Ollama service failed to start"

**Soluzioni:**
1. Verifica che Ollama sia installato: `which ollama`
2. Avvia manualmente: `ollama serve`
3. Controlla i log: `cat ~/.ollama/logs/server.log`

### Modello non trovato

**Sintomi:**
- "Default model llama3.2:3b not found"

**Soluzioni:**
1. Scarica dall'interfaccia web (metodo consigliato)
2. Oppure da terminale: `ollama pull llama3.2:3b`

### Errore durante il download

**Sintomi:**
- Download si blocca o fallisce

**Soluzioni:**
1. Controlla la connessione internet
2. Verifica spazio su disco (i modelli possono essere grandi)
3. Riprova il download
4. Scarica manualmente: `ollama pull nome-modello`

## üé® Personalizzazione

### Cambiare il Modello Predefinito

Modifica il file `/apps/backend/src/server.ts`:

```typescript
const defaultModel = 'llama3.2:3b'; // Cambia qui
```

### Aggiungere Nuovi Modelli Consigliati

Modifica il file `/apps/frontend/src/components/ollama/OllamaModelSelector.tsx`:

```typescript
const RECOMMENDED_MODELS = [
  {
    name: 'tuo-modello',
    displayName: 'Il Tuo Modello',
    description: 'Descrizione del modello',
    recommended: true,
    size: '~XGB'
  },
  // ...
];
```

## üìö Architettura

### Backend (`apps/backend/src/`)

- **`utils/ollamaManager.ts`**: Gestione del ciclo di vita di Ollama
  - Avvio automatico del servizio
  - Verifica installazione
  - Download e gestione modelli

- **`routes/ollamaRoutes.ts`**: API REST per Ollama
  - Status check
  - Model management
  - Download progress tracking

- **`server.ts`**: Integrazione nell'avvio del server
  - Auto-start di Ollama
  - Verifica modelli
  - Graceful shutdown

### Frontend (`apps/frontend/src/`)

- **`components/ollama/OllamaModelSelector.tsx`**: UI per gestione modelli
  - Visualizzazione modelli disponibili
  - Download con progress bar
  - Selezione modello attivo

- **`app/upload/page.tsx`**: Integrazione nel flusso di upload
  - Selettore sempre visibile
  - Passaggio modello al processor

- **`components/evaluation/EvaluationProcessor.tsx`**: Utilizzo modello selezionato
  - Parametro `model` nell'API call

## üîÑ Flusso Completo

```
1. Utente avvia app
   ‚Üì
2. Backend verifica/avvia Ollama
   ‚Üì
3. Backend controlla modelli disponibili
   ‚Üì
4. Utente apre /upload
   ‚Üì
5. Frontend mostra modelli disponibili
   ‚Üì
6. Utente seleziona/scarica modello
   ‚Üì
7. Utente carica documento e registra
   ‚Üì
8. Valutazione usa modello selezionato
   ‚Üì
9. Risultati mostrati all'utente
```

## üìû Supporto

Se incontri problemi:

1. Controlla i log del backend
2. Verifica lo stato su `/api/ollama/status`
3. Consulta i log di Ollama: `~/.ollama/logs/`
4. Apri un issue su GitHub con i dettagli

## üéâ Test Rapido

Per testare tutto subito usando i file nella cartella `attachments/`:

```bash
# 1. Avvia l'app
npm run dev

# 2. Vai su http://localhost:3000/upload

# 3. Nel selettore modelli:
#    - Verifica che llama3.2:3b sia installato o scaricalo

# 4. Carica il documento:
#    - attachments/text.txt

# 5. Carica l'audio:
#    - attachments/record.m4a

# 6. Clicca "Ottieni Feedback"

# 7. Osserva il modello selezionato in azione!
```

---

**Fatto! üéâ**

Ora l'applicazione gestisce automaticamente Ollama e permette di scegliere il modello AI preferito direttamente dall'interfaccia web, senza bisogno di configurazioni manuali!
