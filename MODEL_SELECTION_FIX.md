# Fix: Coordinazione Selezione Modello Frontend-Backend

## üîç Problema Identificato

Durante l'analisi approfondita, ho scoperto che **il parametro `model` non veniva propagato correttamente** dal frontend al backend e all'LLM Ollama.

### Flow Originale (ROTTO):
```
Frontend (EvaluationProcessor)
  ‚Üì passa model: "llama3.2:3b"
  ‚Üì
Frontend Service (evaluationService.ts)
  ‚Üì ignora model, non lo invia
  ‚Üì
Backend Controller (evaluationController.ts)
  ‚Üì riceve options ma non estrae model
  ‚Üì
EvaluationServiceManager
  ‚Üì non accetta model in options
  ‚Üì
LocalEvaluationService
  ‚Üì non riceve model parameter
  ‚Üì
OllamaService.generateChat()
  ‚Üì usa SEMPRE evaluationConfig.model hardcoded ‚ùå
  ‚Üì
Ollama LLM (usa modello sbagliato)
```

### Risultato:
- ‚úÖ Frontend permetteva selezione modello
- ‚ùå Backend ignorava completamente il parametro
- ‚ùå Ollama usava sempre il modello di default configurato

## ‚úÖ Soluzione Implementata

Ho implementato il supporto completo del parametro `model` attraverso **TUTTA** la catena di chiamate.

### Flow Corretto (FUNZIONANTE):
```
Frontend (EvaluationProcessor)
  ‚Üì passa model: "llama3.2:3b" ‚úÖ
  ‚Üì
Frontend Service (evaluationService.ts)
  ‚Üì include model in options ‚úÖ
  ‚Üì
Backend Controller (evaluationController.ts)
  ‚Üì passa options con model ‚úÖ
  ‚Üì
EvaluationServiceManager
  ‚Üì accetta options.model ‚úÖ
  ‚Üì
LocalEvaluationService.evaluatePresentation(model)
  ‚Üì passa model a generateChat() ‚úÖ
  ‚Üì
OllamaService.generateChat(prompt, systemPrompt, model)
  ‚Üì usa model || evaluationConfig.model ‚úÖ
  ‚Üì
Ollama LLM (usa modello selezionato!) ‚úÖ
```

## üìù File Modificati

### 1. **Backend - OllamaService** ([ollamaService.ts](apps/backend/src/services/ollamaService.ts:84))

**PRIMA:**
```typescript
async generateChat(prompt: string, systemPrompt?: string): Promise<string> {
  // ...
  const response = await this.client.chat({
    model: evaluationConfig.model,  // ‚ùå SEMPRE hardcoded
    messages,
    // ...
  });
}
```

**DOPO:**
```typescript
async generateChat(prompt: string, systemPrompt?: string, model?: string): Promise<string> {
  const modelToUse = model || evaluationConfig.model;  // ‚úÖ Usa model passato o fallback

  const response = await this.client.chat({
    model: modelToUse,  // ‚úÖ Dinamico
    messages,
    // ...
  });

  logger.info('Ollama chat generation completed', {
    model: modelToUse,  // ‚úÖ Log del modello usato
    // ...
  });
}
```

### 2. **Backend - LocalEvaluationService** ([localEvaluationService.ts](apps/backend/src/services/localEvaluationService.ts:64))

**PRIMA:**
```typescript
async evaluatePresentation(
  transcription: string,
  relevantChunks: Array<{...}>,
  documentId: string
): Promise<EvaluationResult> {
  // ...
  const rawEvaluation = await this.ollamaService!.generateChat(
    evaluationPrompt,
    this.getSystemPrompt()
    // ‚ùå Nessun model parameter
  );
}
```

**DOPO:**
```typescript
async evaluatePresentation(
  transcription: string,
  relevantChunks: Array<{...}>,
  documentId: string,
  model?: string  // ‚úÖ Nuovo parametro
): Promise<EvaluationResult> {
  const modelToUse = model || evaluationConfig.model;

  logger.info('ü§ñ Sending evaluation request to Ollama', {
    model: modelToUse,  // ‚úÖ Log del modello
    // ...
  });

  const rawEvaluation = await this.ollamaService!.generateChat(
    evaluationPrompt,
    this.getSystemPrompt(),
    modelToUse  // ‚úÖ Passa model parameter
  );
}
```

**Stesso fix per `generateDetailedFeedback()`:**
```typescript
async generateDetailedFeedback(
  transcription: string,
  relevantChunks: Array<{...}>,
  focusArea?: keyof EvaluationCriteria,
  model?: string  // ‚úÖ Nuovo parametro
): Promise<string> {
  const modelToUse = model || evaluationConfig.model;

  const detailedFeedback = await this.ollamaService!.generateChat(
    feedbackPrompt,
    this.getFeedbackSystemPrompt(),
    modelToUse  // ‚úÖ Passa model
  );
}
```

### 3. **Backend - EvaluationServiceManager** ([evaluationService.ts](apps/backend/src/services/evaluationService.ts:23))

**PRIMA:**
```typescript
async evaluatePresentation(
  transcription: string,
  documentId: string,
  options?: {
    maxRelevantChunks?: number;
    minSimilarityScore?: number;
    detailedAccuracyCheck?: boolean;
    // ‚ùå Mancava model
  }
): Promise<{...}> {
  // ...
  const evaluation = await evaluationService.evaluatePresentation(
    transcription,
    relevantChunks,
    documentId
    // ‚ùå Non passava model
  );
}
```

**DOPO:**
```typescript
async evaluatePresentation(
  transcription: string,
  documentId: string,
  options?: {
    maxRelevantChunks?: number;
    minSimilarityScore?: number;
    detailedAccuracyCheck?: boolean;
    model?: string;  // ‚úÖ Aggiunto
  }
): Promise<{...}> {
  // ...
  const evaluation = await evaluationService.evaluatePresentation(
    transcription,
    relevantChunks,
    documentId,
    options?.model  // ‚úÖ Passa model parameter
  );
}
```

### 4. **Backend - AccuracyCheckService** ([accuracyCheckService.ts](apps/backend/src/services/accuracyCheckService.ts:63))

**PRIMA:**
```typescript
async performDetailedAccuracyCheck(
  transcription: string,
  relevantChunks: Array<{...}>,
  documentId: string
): Promise<DetailedAccuracyReport> {
  // ...
  const statements = await this.extractStatements(transcription);
  // ‚ùå Non passava model

  const factCheck = await this.checkStatementAccuracy(
    statement.text,
    documentContext
    // ‚ùå Non passava model
  );
}
```

**DOPO:**
```typescript
async performDetailedAccuracyCheck(
  transcription: string,
  relevantChunks: Array<{...}>,
  documentId: string,
  model?: string  // ‚úÖ Nuovo parametro
): Promise<DetailedAccuracyReport> {
  const modelToUse = model || evaluationConfig.model;

  logger.info('üîç Starting detailed accuracy check', {
    documentId,
    model: modelToUse,  // ‚úÖ Log del modello
    // ...
  });

  const statements = await this.extractStatements(transcription, modelToUse);  // ‚úÖ Passa model

  const factCheck = await this.checkStatementAccuracy(
    statement.text,
    documentContext,
    modelToUse  // ‚úÖ Passa model
  );
}

private async extractStatements(transcription: string, model?: string): Promise<Statement[]> {
  const response = await this.ollamaService!.generateChat(prompt, systemPrompt, model);  // ‚úÖ
}

private async checkStatementAccuracy(
  statement: string,
  documentContext: string,
  model?: string  // ‚úÖ
): Promise<FactCheckResult> {
  const response = await this.ollamaService!.generateChat(prompt, systemPrompt, model);  // ‚úÖ
}
```

### 5. **Backend - Passaggio ad AccuracyCheckService** ([evaluationService.ts](apps/backend/src/services/evaluationService.ts:172))

**PRIMA:**
```typescript
accuracyReport = await accuracyService.performDetailedAccuracyCheck(
  transcription,
  relevantChunks,
  documentId
  // ‚ùå Non passava model
);
```

**DOPO:**
```typescript
logger.info('üîç Starting detailed accuracy check', {
  documentId,
  model: options?.model  // ‚úÖ Log del modello
});

accuracyReport = await accuracyService.performDetailedAccuracyCheck(
  transcription,
  relevantChunks,
  documentId,
  options?.model  // ‚úÖ Passa model parameter
);
```

### 6. **Frontend - evaluationService.ts** ([evaluationService.ts](apps/frontend/src/services/evaluationService.ts:105))

**PRIMA:**
```typescript
body: JSON.stringify({
  transcription,
  documentId,
  options: {
    maxRelevantChunks: options?.maxChunks,
    detailedAccuracyCheck: options?.detailedAccuracyCheck,
    // ‚ùå Mancava model
  }
}),
```

**DOPO:**
```typescript
body: JSON.stringify({
  transcription,
  documentId,
  options: {
    maxRelevantChunks: options?.maxChunks,
    detailedAccuracyCheck: options?.detailedAccuracyCheck,
    model: options?.model,  // ‚úÖ Incluso model
  }
}),
```

## üéØ Punti di Propagazione del Modello

Il parametro `model` ora viene propagato attraverso:

1. ‚úÖ **Frontend UI** ‚Üí `EvaluationProcessor` (gi√† esistente)
2. ‚úÖ **Frontend Service** ‚Üí `evaluationService.evaluatePresentation()` (gi√† esistente)
3. ‚úÖ **API Request** ‚Üí Body con `options.model` (**FIXATO**)
4. ‚úÖ **Backend Controller** ‚Üí `evaluationService.evaluatePresentation()` (gi√† passava options)
5. ‚úÖ **EvaluationServiceManager** ‚Üí Accept `options.model` (**FIXATO**)
6. ‚úÖ **LocalEvaluationService** ‚Üí Accept `model` parameter (**FIXATO**)
7. ‚úÖ **OllamaService** ‚Üí Accept `model` parameter (**FIXATO**)
8. ‚úÖ **AccuracyCheckService** ‚Üí Accept `model` parameter (**FIXATO**)
9. ‚úÖ **Ollama Client** ‚Üí Use dynamic model (**FIXATO**)

## üß™ Testing

Per verificare che il fix funzioni:

### 1. Verifica nei Log

Quando esegui una valutazione, dovresti vedere nei log del backend:

```
[info]: üîç RAG Evaluation - Chunks used for evaluation {
  "documentId": "...",
  "model": "llama3.2:3b",  ‚Üê Modello selezionato!
  ...
}

[info]: ü§ñ Sending evaluation request to Ollama {
  "model": "llama3.2:3b",  ‚Üê Modello usato!
  ...
}

[info]: Ollama chat generation completed {
  "model": "llama3.2:3b",  ‚Üê Conferma modello!
  ...
}
```

### 2. Test Funzionale

```bash
# Test con modello specifico
curl -X POST http://localhost:3001/api/evaluations/evaluate \
  -H "Content-Type: application/json" \
  -d '{
    "transcription": "Test di valutazione con modello personalizzato",
    "documentId": "doc-123",
    "options": {
      "model": "llama3:latest",
      "detailedAccuracyCheck": true
    }
  }'
```

Verifica nei log che il modello usato sia `llama3:latest` e non il default.

### 3. Test Frontend

1. Apri l'app nel browser
2. Nella UI di valutazione, seleziona un modello diverso dal default (es. `llama3:latest`)
3. Esegui una valutazione
4. Controlla i log del backend per vedere quale modello √® stato effettivamente usato

## üìä Impatto

### Prima del Fix:
- ‚ùå Selettore modello nel frontend era **inutile**
- ‚ùå Sempre usato `evaluationConfig.model` (default: `llama3.2:3b`)
- ‚ùå Impossibile testare modelli diversi
- ‚ùå Nessun log del modello effettivamente usato

### Dopo il Fix:
- ‚úÖ Selettore modello nel frontend **funzionante**
- ‚úÖ Modello dinamico in base alla selezione utente
- ‚úÖ Fallback a `evaluationConfig.model` se non specificato
- ‚úÖ Log completi del modello usato in ogni fase
- ‚úÖ Supporto per `detailedAccuracyCheck` con modello personalizzato

## üîß Dettagli Tecnici

### Pattern Utilizzato

In ogni funzione che chiama l'LLM, ho usato questo pattern:

```typescript
async functionName(...args, model?: string) {
  const modelToUse = model || evaluationConfig.model;

  logger.info('Operation starting', { model: modelToUse, ... });

  const result = await this.ollamaService!.generateChat(
    prompt,
    systemPrompt,
    modelToUse  // Passa esplicitamente
  );

  return result;
}
```

### Vantaggi del Pattern:
1. **Backward Compatible**: Se `model` non viene passato, usa il default
2. **Explicit Logging**: Ogni operazione logga il modello usato
3. **No Breaking Changes**: Funziona sia con che senza model parameter
4. **Consistent**: Stesso pattern in tutti i servizi

## üìö Conclusioni

### Problema Risolto:
Il parametro `model` ora viene correttamente propagato da frontend a Ollama attraverso **tutti** i layer:
- Frontend UI ‚úÖ
- Frontend Service ‚úÖ
- API HTTP ‚úÖ
- Backend Controller ‚úÖ
- Evaluation Services ‚úÖ
- Ollama Service ‚úÖ
- LLM Client ‚úÖ

### Verifiche Effettuate:
- ‚úÖ Compilazione senza errori (0 TypeScript errors)
- ‚úÖ Tutti i servizi aggiornati
- ‚úÖ Logging completo implementato
- ‚úÖ Backward compatibility mantenuta

### Come Verificare:
1. Controllare i log del backend durante una valutazione
2. Verificare che il modello nei log corrisponda a quello selezionato
3. Testare con modelli diversi e confermare che vengano usati

### Benefici:
- üéØ **Selezione modello funzionante**: Gli utenti possono ora scegliere quale modello usare
- üîç **Debugging migliorato**: Log chiari mostrano quale modello viene usato
- üß™ **Testing flessibile**: Possibile testare diverse LLM senza cambiare configurazione
- üöÄ **Scalabilit√†**: Facile aggiungere nuovi modelli in futuro
